{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering with Transformers"
      ],
      "metadata": {
        "id": "JGf5XUe1pVUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBIzgOBnpOG5"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_metric\n",
        "from evaluate import load\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_models = {\n",
        "    \"English\":{\n",
        "      \"BERT\": pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\"),\n",
        "      \"DistilBERT\": pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\"),\n",
        "      \"RoBERTa\": pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\"),\n",
        "      \"DeBERTa\": pipeline(\"question-answering\", model=\"timpal0l/mdeberta-v3-base-squad2\"),\n",
        "    },\n",
        "    \"Arabic\": {\n",
        "      \"BERT\": pipeline(\"question-answering\", model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"),\n",
        "      \"XLM-Roberta (Multilingual)\": pipeline(\"question-answering\", model=\"deepset/xlm-roberta-base-squad2\"),\n",
        "      \"XQuAD BERT (Arabic)\": pipeline(\"question-answering\", model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\"),\n",
        "      \"MultiBERT (Arabic)\": pipeline(\"question-answering\", model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\")\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkbkzQpns99Z",
        "outputId": "7d8fc9cb-a50d-4bf1-f886-4912403bd3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at mrm8488/bert-multi-cased-finetuned-xquadv1 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at mrm8488/bert-multi-cased-finetuned-xquadv1 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Some weights of the model checkpoint at mrm8488/bert-multi-cased-finetuned-xquadv1 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"context\": \"Mosen is a fourth year computer science student at MSA university, he loves playing padel and wathcing movies.\",\n",
        "        \"question\": \"What sport does Mosen play?\",\n",
        "        \"answers\": {\"text\": [\"football\"], \"answer_start\": [93]}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"2\",\n",
        "        \"context\": \"The Eiffel Tower is located in Paris, France. It was completed in 1889.\",\n",
        "        \"question\": \"Where is the Eiffel Tower located?\",\n",
        "        \"answers\": {\"text\": [\"Paris, France\"], \"answer_start\": [34]}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"3\",\n",
        "        \"context\": \"Water boils at 100 degrees Celsius under normal atmospheric pressure.\",\n",
        "        \"question\": \"At what temperature does water boil?\",\n",
        "        \"answers\": {\"text\": [\"100 degrees Celsius\"], \"answer_start\": [16]}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"4\",\n",
        "        \"context\": \"Albert Einstein developed the theory of relativity which revolutionized modern physics.\",\n",
        "        \"question\": \"Who developed the theory of relativity?\",\n",
        "        \"answers\": {\"text\": [\"Albert Einstein\"], \"answer_start\": [0]}\n",
        "    }\n",
        "]\n",
        "arabic_test_cases = [\n",
        "    {\n",
        "        \"id\": \"ar1\",\n",
        "        \"context\": \"Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ± ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§ Ù…Ù† Ø­ÙŠØ« Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙƒØ§Ù†.\",\n",
        "        \"question\": \"Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ\",\n",
        "        \"answers\": {\"text\": [\"Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\"], \"answer_start\": [0]}\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"ar2\",\n",
        "        \"context\": \"ÙˆÙ„Ø¯ Ø£Ø­Ù…Ø¯ Ø²ÙˆÙŠÙ„ ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ø¯Ù…Ù†Ù‡ÙˆØ± Ø§Ù„Ù…ØµØ±ÙŠØ© ÙˆØ­ØµÙ„ Ø¹Ù„Ù‰ Ø¬Ø§Ø¦Ø²Ø© Ù†ÙˆØ¨Ù„ ÙÙŠ Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¡.\",\n",
        "        \"question\": \"ÙÙŠ Ø£ÙŠ Ù…Ø¯ÙŠÙ†Ø© ÙˆÙÙ„Ø¯ Ø£Ø­Ù…Ø¯ Ø²ÙˆÙŠÙ„ØŸ\",\n",
        "        \"answers\": {\"text\": [\"Ø¯Ù…Ù†Ù‡ÙˆØ±\"], \"answer_start\": [21]}\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "9oMibgj_2KdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate each model\n",
        "## on english test cases\n",
        "print(\"###### Evaluating models on English test cases: ######\")\n",
        "for model_name, qa_pipeline in qa_models[\"English\"].items():\n",
        "    print(f\"\\nğŸ” Evaluating model: {model_name}\")\n",
        "    metric = load(\"squad\")\n",
        "    for case in test_cases:\n",
        "        result = qa_pipeline({\n",
        "            \"context\": case[\"context\"],\n",
        "            \"question\": case[\"question\"]\n",
        "        })\n",
        "\n",
        "        prediction = {\"id\": case[\"id\"], \"prediction_text\": result[\"answer\"]}\n",
        "        reference = {\"id\": case[\"id\"], \"answers\": case[\"answers\"]}\n",
        "\n",
        "        metric.add(prediction=prediction, reference=reference)\n",
        "\n",
        "    final_scores = metric.compute()\n",
        "    print(f\"Exact Match (EM): {final_scores['exact_match']:.2f}\")\n",
        "    print(f\"F1 Score: {final_scores['f1']:.2f}\")\n",
        "\n",
        "## on Arabic test cases\n",
        "print(\"\\n###### Evaluating models on Arabic test cases: ######\")\n",
        "for model_name, qa_pipeline in qa_models[\"Arabic\"].items():\n",
        "    print(f\"\\nğŸ” Evaluating model: {model_name}\")\n",
        "    metric = load(\"squad\")\n",
        "    for case in arabic_test_cases:\n",
        "        result = qa_pipeline({\n",
        "            \"context\": case[\"context\"],\n",
        "            \"question\": case[\"question\"]\n",
        "        })\n",
        "\n",
        "        prediction = {\"id\": case[\"id\"], \"prediction_text\": result[\"answer\"]}\n",
        "        reference = {\"id\": case[\"id\"], \"answers\": case[\"answers\"]}\n",
        "\n",
        "        metric.add(prediction=prediction, reference=reference)\n",
        "\n",
        "    final_scores = metric.compute()\n",
        "    print(f\"Exact Match (EM): {final_scores['exact_match']:.2f}\")\n",
        "    print(f\"F1 Score: {final_scores['f1']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vMPBc8N7stA",
        "outputId": "ed607917-d252-46f7-daf1-5dc291d70a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### Evaluating models on English test cases: ######\n",
            "\n",
            "ğŸ” Evaluating model: BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match (EM): 75.00\n",
            "F1 Score: 75.00\n",
            "\n",
            "ğŸ” Evaluating model: DistilBERT\n",
            "Exact Match (EM): 75.00\n",
            "F1 Score: 75.00\n",
            "\n",
            "ğŸ” Evaluating model: RoBERTa\n",
            "Exact Match (EM): 75.00\n",
            "F1 Score: 75.00\n",
            "\n",
            "ğŸ” Evaluating model: DeBERTa\n",
            "Exact Match (EM): 75.00\n",
            "F1 Score: 75.00\n",
            "\n",
            "###### Evaluating models on Arabic test cases: ######\n",
            "\n",
            "ğŸ” Evaluating model: BERT\n",
            "Exact Match (EM): 100.00\n",
            "F1 Score: 100.00\n",
            "\n",
            "ğŸ” Evaluating model: XLM-Roberta (Multilingual)\n",
            "Exact Match (EM): 100.00\n",
            "F1 Score: 100.00\n",
            "\n",
            "ğŸ” Evaluating model: XQuAD BERT (Arabic)\n",
            "Exact Match (EM): 100.00\n",
            "F1 Score: 100.00\n",
            "\n",
            "ğŸ” Evaluating model: MultiBERT (Arabic)\n",
            "Exact Match (EM): 100.00\n",
            "F1 Score: 100.00\n"
          ]
        }
      ]
    }
  ]
}